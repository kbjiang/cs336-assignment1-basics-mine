{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7b3611",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "- printable vs no appearance: '\\x00' vs '\\x80'\n",
    "- `some encoding -> binary -> another encoding` is a receipe for errors. See https://en.wikipedia.org/wiki/Specials_(Unicode_block)#:~:text=The%20replacement%20character%20%EF%BF%BD%20(often,of%20data%20to%20correct%20symbols. for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682be3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chr(2049))\n",
    "print(list(chr(2049).encode()))\n",
    "print([bin(n) for n in list(chr(2049).encode())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ba2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_bytes_to_unicode() -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Returns a mapping between every possible byte (an integer from 0 to 255) to a\n",
    "    printable unicode string character representation. This function is taken\n",
    "    from the GPT-2 code.\n",
    "\n",
    "    For example, `chr(0)` is `\\x00`, which is an unprintable character:\n",
    "\n",
    "    >>> chr(0)\n",
    "    '\\x00'\n",
    "    >>> print(chr(0))\n",
    "\n",
    "    As a result, this function returns a dictionary `d` where `d[0]` returns `Ä€`.\n",
    "    The bytes that are visually printable keep their original string representation [1].\n",
    "    For example, `chr(33)` returns `!`, and so accordingly `d[33]` returns `!`.\n",
    "    Note in particular that the space character `chr(32)` becomes `d[32]`, which\n",
    "    returns 'Ä '.\n",
    "\n",
    "    For unprintable characters, the function shifts takes the integer representing\n",
    "    the Unicode code point of that character (returned by the Python `ord`) function\n",
    "    and shifts it by 256. For example, `ord(\" \")` returns `32`, so the the space character\n",
    "    ' ' is shifted to `256 + 32`. Since `chr(256 + 32)` returns `Ä `, we use that as the\n",
    "    string representation of the space.\n",
    "\n",
    "    This function can simplify the BPE implementation and makes it slightly easier to\n",
    "    manually inspect the generated merges after they're serialized to a file.\n",
    "    \"\"\"\n",
    "    # These 188 integers can used as-is, since they are not whitespace or control characters.\n",
    "    # See https://www.ssec.wisc.edu/~tomw/java/unicode.html.\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"Â¡\"), ord(\"Â¬\") + 1)) + list(range(ord(\"Â®\"), ord(\"Ã¿\") + 1))\n",
    "    cs = bs[:]\n",
    "    # now get the representations of the other 68 integers that do need shifting\n",
    "    # each will get mapped chr(256 + n), where n will grow from 0...67 in the loop\n",
    "    # Get printable representations of the remaining integers 68 integers.\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            # If this integer isn't in our list of visually-representable\n",
    "            # charcters, then map it to the next nice character (offset by 256)\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    characters = [chr(n) for n in cs]\n",
    "    d = dict(zip(bs, characters))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pretoken_counts(pretoken_counts, pair_counts, max_pair):\n",
    "    print(\"==========\")\n",
    "    new_pretoken_counts = {}\n",
    "    new_pair_counts = dict(pair_counts) \n",
    "    for byte_tup, byte_tup_count in pretoken_counts.items():\n",
    "        i = 0\n",
    "        new_byte_tup = []\n",
    "        while i < len(byte_tup)-1:\n",
    "            cur = (byte_tup[i], byte_tup[i+1])\n",
    "            if cur == max_pair:\n",
    "                new_byte_tup.append(b\"\".join(max_pair))\n",
    "                new_pair_counts[max_pair] -= byte_tup_count\n",
    "                # when current pair is max_pair, always affect proceeding pair\n",
    "                prev = (byte_tup[i-1], byte_tup[i]) if i > 0 else None\n",
    "                new_pair_counts[prev] = new_pair_counts.get(prev, 0) - byte_tup_count\n",
    "                i += 2\n",
    "            else:\n",
    "                # when not max_pair, just take the element as is\n",
    "                new_byte_tup.append(byte_tup[i])\n",
    "                # need to look into previous two elements\n",
    "                check = (byte_tup[i-2], byte_tup[i-1]) if i > 1 else None\n",
    "                if check == max_pair:\n",
    "                    prev = (byte_tup[i-1], byte_tup[i])\n",
    "                    new_pair_counts[prev] = new_pair_counts.get(prev, 0) - byte_tup_count\n",
    "                i += 1\n",
    "        if i == len(byte_tup) - 1:\n",
    "            new_byte_tup.append(byte_tup[i])\n",
    "        # update pretoken counts\n",
    "        new_byte_tup = tuple(new_byte_tup)\n",
    "        new_pretoken_counts[new_byte_tup] = new_pretoken_counts.get(new_byte_tup, 0) + byte_tup_count\n",
    "\n",
    "        if new_byte_tup != byte_tup:\n",
    "            i = 0\n",
    "            while i < len(new_byte_tup) - 1:\n",
    "                pair = (new_byte_tup[i], new_byte_tup[i+1])\n",
    "                if b\"\".join(max_pair) in pair:\n",
    "                    new_pair_counts[pair] = new_pair_counts.get(pair, 0) + byte_tup_count\n",
    "                i += 1\n",
    "    new_pair_counts = {k:v for k, v in new_pair_counts.items() if v > 0}\n",
    "    return new_pretoken_counts, new_pair_counts, get_max_pair(new_pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_one_tuple(byte_tup, max_pair):\n",
    "    \"\"\"Optimized version using list operations instead of tuple concatenation\"\"\"\n",
    "    # if len(byte_tup) == 2:\n",
    "    #     if byte_tup == max_pair:\n",
    "    #         return (b\"\".join(max_pair),), [0], None, None\n",
    "    #     else:\n",
    "    #         return byte_tup, None, None, None\n",
    "    # if len(byte_tup) == 3:\n",
    "    #     if byte_tup[:2] == max_pair:\n",
    "    #         return (b\"\".join(max_pair), byte_tup[2]), [0, 1], none, [0]\n",
    "    #     if byte_tup[-2:] == max_pair:\n",
    "    #         return (byte_tup[0], b\"\".join(max_pair)), [0, 1], [0], none\n",
    "    \n",
    "    max_pair_0, max_pair_1 = max_pair  # Unpack once\n",
    "    merged_token = max_pair_0 + max_pair_1  # Pre-compute joined bytes\n",
    "\n",
    "    if len(byte_tup) == 1:\n",
    "        return byte_tup, None, None, None\n",
    "\n",
    "    merged_byte_tup = b\"\".join(byte_tup)\n",
    "    if merged_token not in merged_byte_tup:\n",
    "        return byte_tup, None, None, None\n",
    "    \n",
    "\n",
    "    result = []\n",
    "    ids = []\n",
    "    i = 0\n",
    "    while i < len(byte_tup):\n",
    "        if (i < len(byte_tup) - 1 and \n",
    "            byte_tup[i] == max_pair_0 and \n",
    "            byte_tup[i + 1] == max_pair_1):\n",
    "            # Merge the pair\n",
    "            result.append(merged_token)\n",
    "            ids.append(i)\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(byte_tup[i])\n",
    "            i += 1\n",
    "\n",
    "    assert ids is not None, \"something is wrong with `merge_one_tuple` function.\"\n",
    "    ids_prev = [i-1 for i in ids if i > 0]\n",
    "    ids_post = [i+1 for i in ids if i < len(byte_tup)-2]\n",
    "    assert set(ids_prev).intersection(set(ids_post)) == set(), \"`ids_prev` and `ids_post` should have no overlap!\"\n",
    "\n",
    "    ids2rm = set(ids).union(set(ids_prev), set(ids_post))\n",
    "        \n",
    "    return tuple(result), sorted(ids2rm), ids_prev, ids_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_one_tuple((b'l', b'o', b'w'), (b'l', b'o'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pretoken_counts(pretoken_counts, pair_counts, max_pair):\n",
    "    \"\"\"\n",
    "    Optimized version that incrementally updates pair counts instead of \n",
    "    recalculating everything from scratch. Only pairs that overlap with \n",
    "    the merged pair need to have their counts updated.\n",
    "    \"\"\"\n",
    "    new_pretoken_counts = {}\n",
    "    new_pair_counts = dict(pair_counts)  # Faster than .copy()\n",
    "    \n",
    "    # # Remove the merged pair from pair counts\n",
    "    # new_pair_counts.pop(max_pair, None)\n",
    "    \n",
    "    for byte_tup, byte_tup_count in pretoken_counts.items():\n",
    "        new_byte_tup, ids2rm, ids_prev, ids_post = merge_one_tuple(byte_tup, max_pair)\n",
    "        new_pretoken_counts[new_byte_tup] = new_pretoken_counts.get(new_byte_tup, 0) + byte_tup_count\n",
    "        \n",
    "        # Only update pair counts for sequences that actually changed\n",
    "        if ids2rm:\n",
    "            # print(byte_tup)\n",
    "            # print(ids2rm)\n",
    "            for i in ids2rm:\n",
    "                pair = (byte_tup[i], byte_tup[i+1])\n",
    "                count = new_pair_counts.get(pair, 0) - byte_tup_count\n",
    "                if count > 0:\n",
    "                    new_pair_counts[pair] = count\n",
    "                else:\n",
    "                    new_pair_counts.pop(pair)\n",
    "            \n",
    "        if ids_prev:\n",
    "            # Add new pair counts for the merged sequence\n",
    "            for i in ids_prev:\n",
    "                new_pair = (byte_tup[i], b\"\".join(max_pair))\n",
    "                new_pair_counts[new_pair] = new_pair_counts.get(new_pair, 0) + byte_tup_count\n",
    "        if ids_post:\n",
    "            # Add new pair counts for the merged sequence\n",
    "            for i in ids_post:\n",
    "                new_pair = (b\"\".join(max_pair), byte_tup[i+1])\n",
    "                new_pair_counts[new_pair] = new_pair_counts.get(new_pair, 0) + byte_tup_count\n",
    "    \n",
    "    # new_pair_counts = {k:v for k, v in new_pair_counts.items() if v > 0}\n",
    "    return new_pretoken_counts, new_pair_counts, get_max_pair(new_pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01759b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\n",
    "from train_bpe_test import get_max_pair\n",
    "from collections import Counter\n",
    "pretokens = doc.split()\n",
    "pretoken_counts = Counter(pretokens)\n",
    "pretoken_counts = {tuple(bytes([b]) for b in k.encode()):v for k,v in pretoken_counts.items()}\n",
    "print(pretoken_counts)\n",
    "pair_counts = get_pair_counts(pretoken_counts)\n",
    "max_pair = get_max_pair(pair_counts)\n",
    "max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee64a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptc, prc = pretoken_counts.copy(), pair_counts.copy()\n",
    "max_pair = get_max_pair(prc)\n",
    "print(ptc)\n",
    "print(prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(max_pair)\n",
    "    ptc, prc,max_pair = merge_pretoken_counts(ptc, prc, max_pair)\n",
    "    print()\n",
    "    print(ptc)\n",
    "    print(prc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_one_tuple((b'l', b'o', b'w', b'e', b'r'), (b'o', b'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ae029",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(max_pair)\n",
    "    ptc, prc,max_pair = merge_pretoken_counts(ptc, prc, max_pair)\n",
    "    print()\n",
    "    print(ptc)\n",
    "    print(prc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52736d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(max_pair)\n",
    "    ptc, prc,max_pair = merge_pretoken_counts(ptc, prc, max_pair)\n",
    "    print()\n",
    "    print(ptc)\n",
    "    print(prc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(max_pair)\n",
    "    ptc, prc,max_pair = merge_pretoken_counts(ptc, prc, max_pair)\n",
    "    print()\n",
    "    print(ptc)\n",
    "    print(prc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_one_tuple((b'l', b'o', b'w'), (b'o', b'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd4e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b081713",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cde7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_one_tuple((b'l', b'o', b'w'), (b'o', b'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_max_pair({(b'low',): 5, (b'low', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', b'e', b'west'): 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c86631",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4932346",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ptc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ptc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e50e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b61d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29165b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bt, bc in ptc.items():\n",
    "    i = 0\n",
    "    while i < len(bt) - 1:\n",
    "        old_pair = (bt[i], bt[i+1])\n",
    "        print(old_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55aefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "('a', 'b') > chr(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ptc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2678b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1a0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d640fa95",
   "metadata": {},
   "source": [
    "### 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a505b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('Ä ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pretoken(pretoken, pair):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(pretoken):\n",
    "        if (i < len(pretoken) - 1 and \n",
    "            pretoken[i] == pair[0] and \n",
    "            pretoken[i + 1] == pair[1]):\n",
    "            # Merge the pair\n",
    "            result.append(b\"\".join(pair))\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(pretoken[i])\n",
    "            i += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0aa990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../tests\")\n",
    "from common import gpt2_bytes_to_unicode\n",
    "import json\n",
    "\n",
    "def get_tokenizer_from_vocab_merges_path(\n",
    "    vocab_path: str | os.PathLike,\n",
    "    merges_path: str | os.PathLike,\n",
    "    special_tokens: list[str] | None = None,\n",
    "):\n",
    "    gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}\n",
    "    with open(vocab_path) as vocab_f:\n",
    "        gpt2_vocab = json.load(vocab_f)\n",
    "    gpt2_bpe_merges = []\n",
    "    with open(merges_path) as f:\n",
    "        for line in f:\n",
    "            cleaned_line = line.rstrip()\n",
    "            if cleaned_line and len(cleaned_line.split(\" \")) == 2:\n",
    "                gpt2_bpe_merges.append(tuple(cleaned_line.split(\" \")))\n",
    "    # The GPT-2 tokenizer uses a remapped unicode encoding for bytes. Let's\n",
    "    # just return the original bytes, so we don't force students to use\n",
    "    # any particular encoding scheme.\n",
    "    vocab = {\n",
    "        gpt2_vocab_index: bytes([gpt2_byte_decoder[token] for token in gpt2_vocab_item])\n",
    "        for gpt2_vocab_item, gpt2_vocab_index in gpt2_vocab.items()\n",
    "    }\n",
    "    # If any of the special tokens don't exist in the vocab, append them to the vocab.\n",
    "    if special_tokens:\n",
    "        for special_token in special_tokens:\n",
    "            byte_encoded_special_token = special_token.encode(\"utf-8\")\n",
    "            if byte_encoded_special_token not in set(vocab.values()):\n",
    "                vocab[len(vocab)] = byte_encoded_special_token\n",
    "\n",
    "    merges = [\n",
    "        (\n",
    "            bytes([gpt2_byte_decoder[token] for token in merge_token_1]),\n",
    "            bytes([gpt2_byte_decoder[token] for token in merge_token_2]),\n",
    "        )\n",
    "        for merge_token_1, merge_token_2 in gpt2_bpe_merges\n",
    "    ]\n",
    "    # return Tokenizer(vocab, merges, special_tokens)\n",
    "    return vocab, merges, special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "VOCAB_PATH = \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/fixtures/gpt2_vocab.json\"\n",
    "MERGES_PATH = \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/fixtures/gpt2_merges.txt\"\n",
    "VOCAB, MERGES, sptok = get_tokenizer_from_vocab_merges_path(VOCAB_PATH, MERGES_PATH)\n",
    "\n",
    "def split_by_special_tokens(special_tokens, text):\n",
    "    if not special_tokens:\n",
    "        return [text]\n",
    "    escaped_patterns = [re.escape(p) for p in sorted(special_tokens, key=len, reverse=True)]\n",
    "    pattern = f\"({'|'.join(escaped_patterns)})\"\n",
    "    return re.split(pattern, text)\n",
    "\n",
    "import tiktoken\n",
    "reference_tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a3366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910018a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Iterator\n",
    "import regex as re\n",
    "import json\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: dict[int, bytes],\n",
    "        merges: Iterable[tuple[bytes, bytes]],\n",
    "        special_tokens: list[str] | None = None\n",
    "    ):\n",
    "        self.vocab = vocab if vocab else {}\n",
    "        self.merges = merges if merges else []\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath:str, merges_filepath:str, special_tokens: list[str] | None=None):\n",
    "        gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}\n",
    "        with open(vocab_filepath) as vocab_f:\n",
    "            gpt2_vocab = json.load(vocab_f)\n",
    "        gpt2_bpe_merges = []\n",
    "        with open(merges_filepath) as f:\n",
    "            for line in f:\n",
    "                cleaned_line = line.rstrip()\n",
    "                if cleaned_line and len(cleaned_line.split(\" \")) == 2:\n",
    "                    gpt2_bpe_merges.append(tuple(cleaned_line.split(\" \")))\n",
    "        # The GPT-2 tokenizer uses a remapped unicode encoding for bytes. Let's\n",
    "        # just return the original bytes, so we don't force students to use\n",
    "        # any particular encoding scheme.\n",
    "        vocab = {\n",
    "            gpt2_vocab_index: bytes([gpt2_byte_decoder[token] for token in gpt2_vocab_item])\n",
    "            for gpt2_vocab_item, gpt2_vocab_index in gpt2_vocab.items()\n",
    "        }\n",
    "        # If any of the special tokens don't exist in the vocab, append them to the vocab.\n",
    "        if special_tokens:\n",
    "            for special_token in special_tokens:\n",
    "                byte_encoded_special_token = special_token.encode(\"utf-8\")\n",
    "                if byte_encoded_special_token not in set(vocab.values()):\n",
    "                    vocab[len(vocab)] = byte_encoded_special_token\n",
    "\n",
    "        merges = [\n",
    "            (\n",
    "                bytes([gpt2_byte_decoder[token] for token in merge_token_1]),\n",
    "                bytes([gpt2_byte_decoder[token] for token in merge_token_2]),\n",
    "            )\n",
    "            for merge_token_1, merge_token_2 in gpt2_bpe_merges\n",
    "        ]\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        vocab_reversed = {v:k for k,v in self.vocab.items()}\n",
    "        pretokens = re.findall(PAT, text)\n",
    "        chunks = split_by_special_tokens(self.special_tokens, text)\n",
    "        tokens = []\n",
    "        for chunk in chunks:\n",
    "            if chunk in self.special_tokens:\n",
    "                tokens.append(chunk.encode())\n",
    "                continue\n",
    "            pretokens = re.findall(PAT, chunk)\n",
    "            for pretoken in pretokens:\n",
    "                pretoken  = [bytes([b]) for b in pretoken.encode()]\n",
    "                while len(pretoken) >= 2:\n",
    "                    pairs = list(zip(pretoken[:-1], pretoken[1:]))\n",
    "                    try:\n",
    "                        pid = min([self.merges.index(p) for p in pairs if p in self.merges])\n",
    "                        pair = self.merges[pid]\n",
    "                        pretoken = update_pretoken(pretoken, pair)\n",
    "                    except ValueError:\n",
    "                        break\n",
    "                tokens.extend(pretoken)\n",
    "        return [vocab_reversed[token] for token in tokens]\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for text in iterable:\n",
    "            yield from self.encode(text)\n",
    "\n",
    "    def decode(self, ids: list[int]):\n",
    "        return b\"\".join([self.vocab[i] for i in ids]).decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67752b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_files(VOCAB_PATH, MERGES_PATH, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ðŸ™ƒ<|endoftext|>\"\n",
    "ids = tokenizer.encode(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ðŸ™ƒ<|endoftext|>\"\n",
    "# test_string = \"Hello how <|endoftext|><|endoftext|> are u? ðŸ™ƒ<|endoftext|>\"\n",
    "# ids = tokenizer.encode(test_string)\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "text = test_string\n",
    "vocab_reversed = {v:k for k,v in VOCAB.items()}\n",
    "chunks = split_by_special_tokens(tokenizer.special_tokens, text)\n",
    "tokens = []\n",
    "for chunk in chunks:\n",
    "    if chunk in tokenizer.special_tokens:\n",
    "        tokens.append(chunk.encode())\n",
    "        continue\n",
    "    pretokens = re.findall(PAT, chunk)\n",
    "    for pretoken in pretokens:\n",
    "        pretoken  = [bytes([b]) for b in pretoken.encode()]\n",
    "        while len(pretoken) >= 2:\n",
    "            pairs = list(zip(pretoken[:-1], pretoken[1:]))\n",
    "            try:\n",
    "                pid = min([tokenizer.merges.index(p) for p in pairs if p in tokenizer.merges])\n",
    "                pair = tokenizer.merges[pid]\n",
    "                pretoken = update_pretoken(pretoken, pair)\n",
    "            except ValueError:\n",
    "                break\n",
    "        tokens.extend(pretoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e378818",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_ids = reference_tokenizer.encode(test_string, allowed_special={\"<|endoftext|>\"})\n",
    "print(reference_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb9acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_tokenizer.decode(reference_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5856dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(127).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b\"\\x3c\".decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b215652",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6825ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Ã²\".encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d286b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "b'\\xc3\\xb3'.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34611f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB[127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB[2634]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd897aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('\\xa9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7689db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b'\\xc3\\xb2'.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef82d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ids:\n",
    "    print(VOCAB[i].decode(\"utf-8\", errors=\"replace\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba28b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB[127].decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97daedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06eaaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretoken = re.findall(PAT, test_string)[0]\n",
    "print(pretoken)\n",
    "\n",
    "pretoken = [bytes([b]) for b in test_string.encode()]\n",
    "print(pretoken)\n",
    "i = 0\n",
    "while len(pretoken) >= 2:\n",
    "    pairs = list(zip(pretoken[:-1], pretoken[1:]))\n",
    "    try:\n",
    "        pid = min([MERGES.index(p) for p in pairs if p in MERGES])\n",
    "    except ValueError:\n",
    "        break\n",
    "    pair = MERGES[pid]\n",
    "    pretoken = update_pretoken(pretoken, pair)\n",
    "    print(pretoken)\n",
    "\n",
    "# if idx_cur > len(MERGES):\n",
    "#     print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a66698",
   "metadata": {},
   "source": [
    "> Good example of why we cannot use vocab to do encoding. This goes into post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2cbd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(self, text: str) -> list[int]:\n",
    "    vocab_reversed = {v:k for k,v in self.vocab.items()}\n",
    "    pretokens = re.findall(PAT, text)\n",
    "    tokens = []\n",
    "    for pretoken in pretokens:\n",
    "        pretoken  = [bytes([b]) for b in pretoken.encode()]\n",
    "        token = pretoken[0]\n",
    "        i = 1\n",
    "        while i < len(pretoken):\n",
    "            token_ = b\"\".join((token, pretoken[i]))\n",
    "            if token_ in vocab_reversed:\n",
    "                token = token_\n",
    "                i += 1\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "                token = pretoken[i]\n",
    "                i += 1\n",
    "        tokens.append(token)\n",
    "    return [vocab_reversed[token] for token in tokens]\n",
    "\n",
    "import tiktoken\n",
    "reference_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "test_string = \"HÃ©llÃ² hÃ´w are Ã¼? ðŸ™ƒ\"\n",
    "\n",
    "reference_ids = reference_tokenizer.encode(test_string)\n",
    "ids = tokenizer.encode(test_string)\n",
    "assert ids != reference_ids\n",
    "\n",
    "print(VOCAB[220], VOCAB[8582])\n",
    "print(VOCAB[12520])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a02288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the sample file\n",
    "with open(\"../tests/fixtures/tinystories_sample.txt\") as f:\n",
    "    # Use encode_iterable with the file handle. This returns a generator.\n",
    "    token_generator = tokenizer.encode_iterable(f)\n",
    "    \n",
    "    # Let's iterate through the generator and print the first 20 token IDs\n",
    "    print(\"First 20 token IDs:\")\n",
    "    for i, token_id in enumerate(token_generator):\n",
    "        if i >= 20:\n",
    "            break\n",
    "        print(token_id, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5690dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_string = reference_tokenizer.encode(test_string, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d53b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_string.count(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a125106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "min([MERGES.index(p) for p in pair_counts if p in MERGES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "min([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(pair_counts, key=lambda p: MERGES.index(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGES.index((b'\\xc3', b'\\xa9'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b564cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de5598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    vocab_reversed = {v:k for k,v in VOCAB.items()}\n",
    "    pretokens = re.findall(PAT, text)\n",
    "    print(pretokens)\n",
    "    tokens = []\n",
    "    for pretoken in pretokens:\n",
    "        pretoken  = [bytes([b]) for b in pretoken.encode()]\n",
    "        print(pretoken)\n",
    "        token = pretoken[0]\n",
    "        i = 1\n",
    "        while i < len(pretoken):\n",
    "            token_tmp = b\"\".join((token, pretoken[i]))\n",
    "            if token_tmp in vocab_reversed:\n",
    "                token = token_tmp\n",
    "                i += 1\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "                print(token)\n",
    "                token = pretoken[i]\n",
    "                i += 1\n",
    "        tokens.append(token)\n",
    "        print(token)\n",
    "    return [vocab_reversed[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24af9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(b'Hel', b'l') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1020fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34299a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = {v:k for k,v in VOCAB.items()}\n",
    "vr[b'He']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5fc038",
   "metadata": {},
   "source": [
    "> could miss merges like (b' ', b'\\x0b9\\x011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f9b045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/fixtures/gpt2_vocab.json\"\n",
    "MERGES_PATH = \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/fixtures/gpt2_merges.txt\"\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def test_ascii_string_matches_tiktoken():\n",
    "    reference_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    # tokenizer = get_tokenizer_from_vocab_merges_path(\n",
    "    tokenizer = Tokenizer.from_files(\n",
    "        VOCAB_PATH, MERGES_PATH, [\"<|endoftext|>\"]\n",
    "    )\n",
    "    test_string = \"Hello, how are you?\"\n",
    "\n",
    "    reference_ids = reference_tokenizer.encode(test_string)\n",
    "    print(reference_ids)\n",
    "    ids = tokenizer.encode(test_string)\n",
    "    # assert ids == reference_ids\n",
    "\n",
    "    tokenized_string = [tokenizer.decode([x]) for x in ids]\n",
    "\n",
    "    return tokenized_string\n",
    "    # assert tokenized_string == [\"Hello\", \",\", \" how\", \" are\", \" you\", \"?\"]\n",
    "\n",
    "    # assert tokenizer.decode(ids) == test_string\n",
    "    # assert reference_tokenizer.decode(reference_ids) == test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ascii_string_matches_tiktoken()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df59603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3309327",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(token_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(\" the bananas are green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7705b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = \"<|endoftext|>\"\n",
    "with open(\"../data/TinyStoriesV2-GPT4-valid.txt\", \"rb\") as f:\n",
    "    doc = f.read().split(special_token.encode())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jkj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.loads(json.dumps({1:vocab[1].decode()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822f082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381d2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "# for (p0, p1), idx in merges.items():\n",
    "#     vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(bytes([28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7620aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes([68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aaba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(b'&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324b396",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "- `train_bpe_tinystoires`\n",
    "    - Current memory usage: 5.24 MB\n",
    "    - Peak memory usage: 116.74 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18274d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b'a' in (b'a' + b'\\x80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b'abc'[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf464893",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(b'bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f74380",
   "metadata": {},
   "outputs": [],
   "source": [
    "b'ab'.replace(b'a', b'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "def _update_byte_tuple(byte_tuple: Iterable[bytes], merge_loc: int):\n",
    "    \"\"\"\n",
    "    Merge the byte tuple at the merge location.\n",
    "    \"\"\"\n",
    "    assert len(byte_tuple) > 1, \"Cannot merge a byte tuple with length less than 2.\"\n",
    "    prefix = byte_tuple[:merge_loc]\n",
    "    tomerge = byte_tuple[merge_loc:merge_loc+2]\n",
    "    suffix = byte_tuple[merge_loc+2:]\n",
    "    new_byte_tuple = prefix + (b\"\".join(tomerge),) + suffix\n",
    "    return new_byte_tuple, prefix, suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_tuple = tuple(bytes([c]) for c in 'xyz'.encode())\n",
    "# tuple(bytes([b]) for b in pretoken)\n",
    "_update_byte_tuple(byte_tuple, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ce616",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839df03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(b'xyz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
