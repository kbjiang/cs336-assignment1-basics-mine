{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7b3611",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "- printable vs no appearance: '\\x00' vs '\\x80'\n",
    "- `some encoding -> binary -> another encoding` is a receipe for errors. See https://en.wikipedia.org/wiki/Specials_(Unicode_block)#:~:text=The%20replacement%20character%20%EF%BF%BD%20(often,of%20data%20to%20correct%20symbols. for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "682be3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à \n",
      "[224, 160, 129]\n",
      "['0b11100000', '0b10100000', '0b10000001']\n"
     ]
    }
   ],
   "source": [
    "print(chr(2049))\n",
    "print(list(chr(2049).encode()))\n",
    "print([bin(n) for n in list(chr(2049).encode())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae84c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "795ba2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_bytes_to_unicode() -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Returns a mapping between every possible byte (an integer from 0 to 255) to a\n",
    "    printable unicode string character representation. This function is taken\n",
    "    from the GPT-2 code.\n",
    "\n",
    "    For example, `chr(0)` is `\\x00`, which is an unprintable character:\n",
    "\n",
    "    >>> chr(0)\n",
    "    '\\x00'\n",
    "    >>> print(chr(0))\n",
    "\n",
    "    As a result, this function returns a dictionary `d` where `d[0]` returns `Ä€`.\n",
    "    The bytes that are visually printable keep their original string representation [1].\n",
    "    For example, `chr(33)` returns `!`, and so accordingly `d[33]` returns `!`.\n",
    "    Note in particular that the space character `chr(32)` becomes `d[32]`, which\n",
    "    returns 'Ä '.\n",
    "\n",
    "    For unprintable characters, the function shifts takes the integer representing\n",
    "    the Unicode code point of that character (returned by the Python `ord`) function\n",
    "    and shifts it by 256. For example, `ord(\" \")` returns `32`, so the the space character\n",
    "    ' ' is shifted to `256 + 32`. Since `chr(256 + 32)` returns `Ä `, we use that as the\n",
    "    string representation of the space.\n",
    "\n",
    "    This function can simplify the BPE implementation and makes it slightly easier to\n",
    "    manually inspect the generated merges after they're serialized to a file.\n",
    "    \"\"\"\n",
    "    # These 188 integers can used as-is, since they are not whitespace or control characters.\n",
    "    # See https://www.ssec.wisc.edu/~tomw/java/unicode.html.\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"Â¡\"), ord(\"Â¬\") + 1)) + list(range(ord(\"Â®\"), ord(\"Ã¿\") + 1))\n",
    "    cs = bs[:]\n",
    "    # now get the representations of the other 68 integers that do need shifting\n",
    "    # each will get mapped chr(256 + n), where n will grow from 0...67 in the loop\n",
    "    # Get printable representations of the remaining integers 68 integers.\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            # If this integer isn't in our list of visually-representable\n",
    "            # charcters, then map it to the next nice character (offset by 256)\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    characters = [chr(n) for n in cs]\n",
    "    d = dict(zip(bs, characters))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640fa95",
   "metadata": {},
   "source": [
    "### 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0384af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_bpe import train_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62516a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = \"/home/azureuser/02-fun/cs336-assignment1-basics/data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "input_path = \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/fixtures/corpus.en\"\n",
    "# vocab_size = 32_000\n",
    "# vocab_size = 10_000\n",
    "vocab_size = 500\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "num_processes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1384171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trianing started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/243 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:00<00:00, 395.75it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab, merges = train_bpe(\n",
    "    input_path, vocab_size, special_tokens, num_processes\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6718d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_vocab.json\") as f:\n",
    "    vocab_rdbk = json.load(f)\n",
    "    vocab_rdbk = {int(k): v for k,v in vocab_rdbk.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dac4bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges_2save = [tuple(map(decode_w_replace, merge)) for merge in merges]\n",
    "with open(\"test_merges.txt\", \"w\") as f:\n",
    "    for merge in merges_2save:\n",
    "        f.write(f\"{tuple(merge)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1285927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'nd'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"nd\".encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da411001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def decode_w_replace(x: bytes):\n",
    "    return x.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "def save_vocab_and_merges(\n",
    "    vocab: dict[int, bytes],\n",
    "    merges: list[tuple[bytes, bytes]],\n",
    "    vocab_path: str,\n",
    "    merges_path: str\n",
    "):\n",
    "    vocab_2save = {k:decode_w_replace(v) for k,v in vocab.items()}\n",
    "    with open(vocab_path, \"w\") as f:\n",
    "        json.dump(vocab_2save, f)\n",
    "    merges_2save = [tuple(map(decode_w_replace, merge)) for merge in merges]\n",
    "    with open(merges_path, \"w\") as f:\n",
    "        for merge in merges_2save:\n",
    "            f.write(f\"{tuple(merge)}\\n\")\n",
    "\n",
    "save_vocab_and_merges(vocab, merges, \"test_vocab.json\", \"test_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "910018a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Iterator\n",
    "from ast import literal_eval\n",
    "import regex as re\n",
    "import json\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: dict[int, bytes],\n",
    "        merges: Iterable[tuple[bytes, bytes]],\n",
    "        special_tokens: list[str] | None = None\n",
    "    ):\n",
    "        self.vocab = vocab if vocab else {}\n",
    "        self.merges = merges if merges else []\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath:str, merges_filepath:str, special_tokens: list[str] | None=None):\n",
    "        gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}\n",
    "        with open(vocab_filepath) as vocab_f:\n",
    "            gpt2_vocab = json.load(vocab_f)\n",
    "        gpt2_bpe_merges = []\n",
    "        with open(merges_filepath) as f:\n",
    "            for line in f:\n",
    "                cleaned_line = line.rstrip()\n",
    "                if cleaned_line and len(cleaned_line.split(\" \")) == 2:\n",
    "                    gpt2_bpe_merges.append(tuple(cleaned_line.split(\" \")))\n",
    "        # The GPT-2 tokenizer uses a remapped unicode encoding for bytes. Let's\n",
    "        # just return the original bytes, so we don't force students to use\n",
    "        # any particular encoding scheme.\n",
    "        vocab = {\n",
    "            gpt2_vocab_index: bytes([gpt2_byte_decoder[token] for token in gpt2_vocab_item])\n",
    "            for gpt2_vocab_item, gpt2_vocab_index in gpt2_vocab.items()\n",
    "        }\n",
    "        # If any of the special tokens don't exist in the vocab, append them to the vocab.\n",
    "        if special_tokens:\n",
    "            for special_token in special_tokens:\n",
    "                byte_encoded_special_token = special_token.encode(\"utf-8\")\n",
    "                if byte_encoded_special_token not in set(vocab.values()):\n",
    "                    vocab[len(vocab)] = byte_encoded_special_token\n",
    "\n",
    "        merges = [\n",
    "            (\n",
    "                bytes([gpt2_byte_decoder[token] for token in merge_token_1]),\n",
    "                bytes([gpt2_byte_decoder[token] for token in merge_token_2]),\n",
    "            )\n",
    "            for merge_token_1, merge_token_2 in gpt2_bpe_merges\n",
    "        ]\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        vocab_reversed = {v:k for k,v in self.vocab.items()}\n",
    "        pretokens = re.findall(PAT, text)\n",
    "        tokens = []\n",
    "        for pretoken in pretokens:\n",
    "            pretoken  = [bytes([b]) for b in pretoken.encode()]\n",
    "            token = pretoken[0]\n",
    "            i = 1\n",
    "            while i < len(pretoken):\n",
    "                token_ = b\"\".join((token, pretoken[i]))\n",
    "                if token_ in vocab_reversed:\n",
    "                    token = token_\n",
    "                    i += 1\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "                    token = pretoken[i]\n",
    "                    i += 1\n",
    "            tokens.append(token)\n",
    "        return [vocab_reversed[token] for token in tokens]\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for text in iterable:\n",
    "            yield from self.encode(text)\n",
    "\n",
    "    def decode(self, ids: list[int]):\n",
    "        return b\"\".join([self.vocab[i] for i in ids]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "e67752b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_files(VOCAB_PATH, MERGES_PATH, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d76e889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    reference_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    test_string = \"HÃ©llÃ² hÃ´w are Ã¼? ðŸ™ƒ\"\n",
    "\n",
    "    reference_ids = reference_tokenizer.encode(test_string)\n",
    "    ids = tokenizer.encode(test_string)\n",
    "    # assert ids == reference_ids\n",
    "\n",
    "    # assert tokenizer.decode(ids) == test_string\n",
    "    # assert reference_tokenizer.decode(reference_ids) == test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "3f0bd1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 2634, 297, 127, 110, 289, 27083, 86, 389, 6184, 120, 30, 12520, 247, 225]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "188e9b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39,\n",
       " 2634,\n",
       " 297,\n",
       " 127,\n",
       " 110,\n",
       " 289,\n",
       " 27083,\n",
       " 86,\n",
       " 389,\n",
       " 6184,\n",
       " 120,\n",
       " 30,\n",
       " 220,\n",
       " 8582,\n",
       " 247,\n",
       " 225]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "73fa9872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b' ', b'\\xf0\\x9f')"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[220], vocab[8582]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "139a5e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12264\n"
     ]
    }
   ],
   "source": [
    "for i, merge in enumerate(MERGES):\n",
    "    if merge == (b' ', b'\\xf0\\x9f'):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "32ca23e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' \\xf0\\x9f'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[12520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202dcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ba9480af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 703, 389, 345, 30]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17047562",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "99a02288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 token IDs:\n",
      "11, 80, 111, 322, 420, 274, 258, 257, 334, 102, 294, 265, 102, 272, 301, 258, 298, 278, 117, 321, "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c83d78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../tests\")\n",
    "from common import gpt2_bytes_to_unicode\n",
    "\n",
    "def get_tokenizer_from_vocab_merges_path(\n",
    "    vocab_path: str | os.PathLike,\n",
    "    merges_path: str | os.PathLike,\n",
    "    special_tokens: list[str] | None = None,\n",
    "):\n",
    "    gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}\n",
    "    with open(vocab_path) as vocab_f:\n",
    "        gpt2_vocab = json.load(vocab_f)\n",
    "    gpt2_bpe_merges = []\n",
    "    with open(merges_path) as f:\n",
    "        for line in f:\n",
    "            cleaned_line = line.rstrip()\n",
    "            if cleaned_line and len(cleaned_line.split(\" \")) == 2:\n",
    "                gpt2_bpe_merges.append(tuple(cleaned_line.split(\" \")))\n",
    "    # The GPT-2 tokenizer uses a remapped unicode encoding for bytes. Let's\n",
    "    # just return the original bytes, so we don't force students to use\n",
    "    # any particular encoding scheme.\n",
    "    vocab = {\n",
    "        gpt2_vocab_index: bytes([gpt2_byte_decoder[token] for token in gpt2_vocab_item])\n",
    "        for gpt2_vocab_item, gpt2_vocab_index in gpt2_vocab.items()\n",
    "    }\n",
    "    # If any of the special tokens don't exist in the vocab, append them to the vocab.\n",
    "    if special_tokens:\n",
    "        for special_token in special_tokens:\n",
    "            byte_encoded_special_token = special_token.encode(\"utf-8\")\n",
    "            if byte_encoded_special_token not in set(vocab.values()):\n",
    "                vocab[len(vocab)] = byte_encoded_special_token\n",
    "\n",
    "    merges = [\n",
    "        (\n",
    "            bytes([gpt2_byte_decoder[token] for token in merge_token_1]),\n",
    "            bytes([gpt2_byte_decoder[token] for token in merge_token_2]),\n",
    "        )\n",
    "        for merge_token_1, merge_token_2 in gpt2_bpe_merges\n",
    "    ]\n",
    "    # return Tokenizer(vocab, merges, special_tokens)\n",
    "    return vocab, merges, special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b217a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/fixtures/gpt2_vocab.json\"\n",
    "MERGES_PATH = \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/fixtures/gpt2_merges.txt\"\n",
    "VOCAB, MERGES, sptok = get_tokenizer_from_vocab_merges_path(VOCAB_PATH, MERGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "98b769d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(MERGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "58920ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'Hel', b'p')\n",
      "(b'Hel', b'per')\n"
     ]
    }
   ],
   "source": [
    "for merge in MERGES:\n",
    "    if b'Hel' in merge:\n",
    "        print(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f09fed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    vocab_reversed = {v:k for k,v in VOCAB.items()}\n",
    "    pretokens = re.findall(PAT, text)\n",
    "    print(pretokens)\n",
    "    tokens = []\n",
    "    for pretoken in pretokens:\n",
    "        pretoken  = [bytes([b]) for b in pretoken.encode()]\n",
    "        print(pretoken)\n",
    "        token = pretoken[0]\n",
    "        i = 1\n",
    "        while i < len(pretoken):\n",
    "            token_tmp = b\"\".join((token, pretoken[i]))\n",
    "            if token_tmp in vocab_reversed:\n",
    "                token = token_tmp\n",
    "                i += 1\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "                print(token)\n",
    "                token = pretoken[i]\n",
    "                i += 1\n",
    "        tokens.append(token)\n",
    "        print(token)\n",
    "    return [vocab_reversed[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(b'Hel', b'l') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "739080bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', ' how', ' are', ' you', '?']\n",
      "[b'H', b'e', b'l', b'l', b'o']\n",
      "b'Hello'\n",
      "[b',']\n",
      "b','\n",
      "[b' ', b'h', b'o', b'w']\n",
      "b' how'\n",
      "[b' ', b'a', b'r', b'e']\n",
      "b' are'\n",
      "[b' ', b'y', b'o', b'u']\n",
      "b' you'\n",
      "[b'?']\n",
      "b'?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', ' how', ' are', ' you', '?']\n",
      "[b'H', b'e', b'l', b'l', b'o']\n",
      "b'Hello'\n",
      "[b',']\n",
      "b','\n",
      "[b' ', b'h', b'o', b'w']\n",
      "b' how'\n",
      "[b' ', b'a', b'r', b'e']\n",
      "b' are'\n",
      "[b' ', b'y', b'o', b'u']\n",
      "b' you'\n",
      "[b'?']\n",
      "b'?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15496, 11, 703, 389, 345, 30]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "42032887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1544"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr = {v:k for k,v in VOCAB.items()}\n",
    "vr[b'He']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6ba74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "efb3471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/fixtures/gpt2_vocab.json\"\n",
    "MERGES_PATH = \"/home/azureuser/02-fun/cs336-assignment1-basics/tests/fixtures/gpt2_merges.txt\"\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def test_ascii_string_matches_tiktoken():\n",
    "    reference_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    # tokenizer = get_tokenizer_from_vocab_merges_path(\n",
    "    tokenizer = Tokenizer.from_files(\n",
    "        VOCAB_PATH, MERGES_PATH, [\"<|endoftext|>\"]\n",
    "    )\n",
    "    test_string = \"Hello, how are you?\"\n",
    "\n",
    "    reference_ids = reference_tokenizer.encode(test_string)\n",
    "    print(reference_ids)\n",
    "    ids = tokenizer.encode(test_string)\n",
    "    # assert ids == reference_ids\n",
    "\n",
    "    tokenized_string = [tokenizer.decode([x]) for x in ids]\n",
    "\n",
    "    return tokenized_string\n",
    "    # assert tokenized_string == [\"Hello\", \",\", \" how\", \" are\", \" you\", \"?\"]\n",
    "\n",
    "    # assert tokenizer.decode(ids) == test_string\n",
    "    # assert reference_tokenizer.decode(reference_ids) == test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2724479b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1544, 18798, 11, 8169, 86, 610, 68, 27406, 84, 30]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "992f13ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 703, 389, 345, 30]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['He', 'llo', ',', ' ho', 'w', ' ar', 'e', ' yo', 'u', '?']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ascii_string_matches_tiktoken()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6e438213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'<|endoftext|>',\n",
       " 1: b'\\x00',\n",
       " 2: b'\\x01',\n",
       " 3: b'\\x02',\n",
       " 4: b'\\x03',\n",
       " 5: b'\\x04',\n",
       " 6: b'\\x05',\n",
       " 7: b'\\x06',\n",
       " 8: b'\\x07',\n",
       " 9: b'\\x08',\n",
       " 10: b'\\t',\n",
       " 11: b'\\n',\n",
       " 12: b'\\x0b',\n",
       " 13: b'\\x0c',\n",
       " 14: b'\\r',\n",
       " 15: b'\\x0e',\n",
       " 16: b'\\x0f',\n",
       " 17: b'\\x10',\n",
       " 18: b'\\x11',\n",
       " 19: b'\\x12',\n",
       " 20: b'\\x13',\n",
       " 21: b'\\x14',\n",
       " 22: b'\\x15',\n",
       " 23: b'\\x16',\n",
       " 24: b'\\x17',\n",
       " 25: b'\\x18',\n",
       " 26: b'\\x19',\n",
       " 27: b'\\x1a',\n",
       " 28: b'\\x1b',\n",
       " 29: b'\\x1c',\n",
       " 30: b'\\x1d',\n",
       " 31: b'\\x1e',\n",
       " 32: b'\\x1f',\n",
       " 33: b' ',\n",
       " 34: b'!',\n",
       " 35: b'\"',\n",
       " 36: b'#',\n",
       " 37: b'$',\n",
       " 38: b'%',\n",
       " 39: b'&',\n",
       " 40: b\"'\",\n",
       " 41: b'(',\n",
       " 42: b')',\n",
       " 43: b'*',\n",
       " 44: b'+',\n",
       " 45: b',',\n",
       " 46: b'-',\n",
       " 47: b'.',\n",
       " 48: b'/',\n",
       " 49: b'0',\n",
       " 50: b'1',\n",
       " 51: b'2',\n",
       " 52: b'3',\n",
       " 53: b'4',\n",
       " 54: b'5',\n",
       " 55: b'6',\n",
       " 56: b'7',\n",
       " 57: b'8',\n",
       " 58: b'9',\n",
       " 59: b':',\n",
       " 60: b';',\n",
       " 61: b'<',\n",
       " 62: b'=',\n",
       " 63: b'>',\n",
       " 64: b'?',\n",
       " 65: b'@',\n",
       " 66: b'A',\n",
       " 67: b'B',\n",
       " 68: b'C',\n",
       " 69: b'D',\n",
       " 70: b'E',\n",
       " 71: b'F',\n",
       " 72: b'G',\n",
       " 73: b'H',\n",
       " 74: b'I',\n",
       " 75: b'J',\n",
       " 76: b'K',\n",
       " 77: b'L',\n",
       " 78: b'M',\n",
       " 79: b'N',\n",
       " 80: b'O',\n",
       " 81: b'P',\n",
       " 82: b'Q',\n",
       " 83: b'R',\n",
       " 84: b'S',\n",
       " 85: b'T',\n",
       " 86: b'U',\n",
       " 87: b'V',\n",
       " 88: b'W',\n",
       " 89: b'X',\n",
       " 90: b'Y',\n",
       " 91: b'Z',\n",
       " 92: b'[',\n",
       " 93: b'\\\\',\n",
       " 94: b']',\n",
       " 95: b'^',\n",
       " 96: b'_',\n",
       " 97: b'`',\n",
       " 98: b'a',\n",
       " 99: b'b',\n",
       " 100: b'c',\n",
       " 101: b'd',\n",
       " 102: b'e',\n",
       " 103: b'f',\n",
       " 104: b'g',\n",
       " 105: b'h',\n",
       " 106: b'i',\n",
       " 107: b'j',\n",
       " 108: b'k',\n",
       " 109: b'l',\n",
       " 110: b'm',\n",
       " 111: b'n',\n",
       " 112: b'o',\n",
       " 113: b'p',\n",
       " 114: b'q',\n",
       " 115: b'r',\n",
       " 116: b's',\n",
       " 117: b't',\n",
       " 118: b'u',\n",
       " 119: b'v',\n",
       " 120: b'w',\n",
       " 121: b'x',\n",
       " 122: b'y',\n",
       " 123: b'z',\n",
       " 124: b'{',\n",
       " 125: b'|',\n",
       " 126: b'}',\n",
       " 127: b'~',\n",
       " 128: b'\\x7f',\n",
       " 129: b'\\xef\\xbf\\xbd',\n",
       " 130: b'\\xef\\xbf\\xbd',\n",
       " 131: b'\\xef\\xbf\\xbd',\n",
       " 132: b'\\xef\\xbf\\xbd',\n",
       " 133: b'\\xef\\xbf\\xbd',\n",
       " 134: b'\\xef\\xbf\\xbd',\n",
       " 135: b'\\xef\\xbf\\xbd',\n",
       " 136: b'\\xef\\xbf\\xbd',\n",
       " 137: b'\\xef\\xbf\\xbd',\n",
       " 138: b'\\xef\\xbf\\xbd',\n",
       " 139: b'\\xef\\xbf\\xbd',\n",
       " 140: b'\\xef\\xbf\\xbd',\n",
       " 141: b'\\xef\\xbf\\xbd',\n",
       " 142: b'\\xef\\xbf\\xbd',\n",
       " 143: b'\\xef\\xbf\\xbd',\n",
       " 144: b'\\xef\\xbf\\xbd',\n",
       " 145: b'\\xef\\xbf\\xbd',\n",
       " 146: b'\\xef\\xbf\\xbd',\n",
       " 147: b'\\xef\\xbf\\xbd',\n",
       " 148: b'\\xef\\xbf\\xbd',\n",
       " 149: b'\\xef\\xbf\\xbd',\n",
       " 150: b'\\xef\\xbf\\xbd',\n",
       " 151: b'\\xef\\xbf\\xbd',\n",
       " 152: b'\\xef\\xbf\\xbd',\n",
       " 153: b'\\xef\\xbf\\xbd',\n",
       " 154: b'\\xef\\xbf\\xbd',\n",
       " 155: b'\\xef\\xbf\\xbd',\n",
       " 156: b'\\xef\\xbf\\xbd',\n",
       " 157: b'\\xef\\xbf\\xbd',\n",
       " 158: b'\\xef\\xbf\\xbd',\n",
       " 159: b'\\xef\\xbf\\xbd',\n",
       " 160: b'\\xef\\xbf\\xbd',\n",
       " 161: b'\\xef\\xbf\\xbd',\n",
       " 162: b'\\xef\\xbf\\xbd',\n",
       " 163: b'\\xef\\xbf\\xbd',\n",
       " 164: b'\\xef\\xbf\\xbd',\n",
       " 165: b'\\xef\\xbf\\xbd',\n",
       " 166: b'\\xef\\xbf\\xbd',\n",
       " 167: b'\\xef\\xbf\\xbd',\n",
       " 168: b'\\xef\\xbf\\xbd',\n",
       " 169: b'\\xef\\xbf\\xbd',\n",
       " 170: b'\\xef\\xbf\\xbd',\n",
       " 171: b'\\xef\\xbf\\xbd',\n",
       " 172: b'\\xef\\xbf\\xbd',\n",
       " 173: b'\\xef\\xbf\\xbd',\n",
       " 174: b'\\xef\\xbf\\xbd',\n",
       " 175: b'\\xef\\xbf\\xbd',\n",
       " 176: b'\\xef\\xbf\\xbd',\n",
       " 177: b'\\xef\\xbf\\xbd',\n",
       " 178: b'\\xef\\xbf\\xbd',\n",
       " 179: b'\\xef\\xbf\\xbd',\n",
       " 180: b'\\xef\\xbf\\xbd',\n",
       " 181: b'\\xef\\xbf\\xbd',\n",
       " 182: b'\\xef\\xbf\\xbd',\n",
       " 183: b'\\xef\\xbf\\xbd',\n",
       " 184: b'\\xef\\xbf\\xbd',\n",
       " 185: b'\\xef\\xbf\\xbd',\n",
       " 186: b'\\xef\\xbf\\xbd',\n",
       " 187: b'\\xef\\xbf\\xbd',\n",
       " 188: b'\\xef\\xbf\\xbd',\n",
       " 189: b'\\xef\\xbf\\xbd',\n",
       " 190: b'\\xef\\xbf\\xbd',\n",
       " 191: b'\\xef\\xbf\\xbd',\n",
       " 192: b'\\xef\\xbf\\xbd',\n",
       " 193: b'\\xef\\xbf\\xbd',\n",
       " 194: b'\\xef\\xbf\\xbd',\n",
       " 195: b'\\xef\\xbf\\xbd',\n",
       " 196: b'\\xef\\xbf\\xbd',\n",
       " 197: b'\\xef\\xbf\\xbd',\n",
       " 198: b'\\xef\\xbf\\xbd',\n",
       " 199: b'\\xef\\xbf\\xbd',\n",
       " 200: b'\\xef\\xbf\\xbd',\n",
       " 201: b'\\xef\\xbf\\xbd',\n",
       " 202: b'\\xef\\xbf\\xbd',\n",
       " 203: b'\\xef\\xbf\\xbd',\n",
       " 204: b'\\xef\\xbf\\xbd',\n",
       " 205: b'\\xef\\xbf\\xbd',\n",
       " 206: b'\\xef\\xbf\\xbd',\n",
       " 207: b'\\xef\\xbf\\xbd',\n",
       " 208: b'\\xef\\xbf\\xbd',\n",
       " 209: b'\\xef\\xbf\\xbd',\n",
       " 210: b'\\xef\\xbf\\xbd',\n",
       " 211: b'\\xef\\xbf\\xbd',\n",
       " 212: b'\\xef\\xbf\\xbd',\n",
       " 213: b'\\xef\\xbf\\xbd',\n",
       " 214: b'\\xef\\xbf\\xbd',\n",
       " 215: b'\\xef\\xbf\\xbd',\n",
       " 216: b'\\xef\\xbf\\xbd',\n",
       " 217: b'\\xef\\xbf\\xbd',\n",
       " 218: b'\\xef\\xbf\\xbd',\n",
       " 219: b'\\xef\\xbf\\xbd',\n",
       " 220: b'\\xef\\xbf\\xbd',\n",
       " 221: b'\\xef\\xbf\\xbd',\n",
       " 222: b'\\xef\\xbf\\xbd',\n",
       " 223: b'\\xef\\xbf\\xbd',\n",
       " 224: b'\\xef\\xbf\\xbd',\n",
       " 225: b'\\xef\\xbf\\xbd',\n",
       " 226: b'\\xef\\xbf\\xbd',\n",
       " 227: b'\\xef\\xbf\\xbd',\n",
       " 228: b'\\xef\\xbf\\xbd',\n",
       " 229: b'\\xef\\xbf\\xbd',\n",
       " 230: b'\\xef\\xbf\\xbd',\n",
       " 231: b'\\xef\\xbf\\xbd',\n",
       " 232: b'\\xef\\xbf\\xbd',\n",
       " 233: b'\\xef\\xbf\\xbd',\n",
       " 234: b'\\xef\\xbf\\xbd',\n",
       " 235: b'\\xef\\xbf\\xbd',\n",
       " 236: b'\\xef\\xbf\\xbd',\n",
       " 237: b'\\xef\\xbf\\xbd',\n",
       " 238: b'\\xef\\xbf\\xbd',\n",
       " 239: b'\\xef\\xbf\\xbd',\n",
       " 240: b'\\xef\\xbf\\xbd',\n",
       " 241: b'\\xef\\xbf\\xbd',\n",
       " 242: b'\\xef\\xbf\\xbd',\n",
       " 243: b'\\xef\\xbf\\xbd',\n",
       " 244: b'\\xef\\xbf\\xbd',\n",
       " 245: b'\\xef\\xbf\\xbd',\n",
       " 246: b'\\xef\\xbf\\xbd',\n",
       " 247: b'\\xef\\xbf\\xbd',\n",
       " 248: b'\\xef\\xbf\\xbd',\n",
       " 249: b'\\xef\\xbf\\xbd',\n",
       " 250: b'\\xef\\xbf\\xbd',\n",
       " 251: b'\\xef\\xbf\\xbd',\n",
       " 252: b'\\xef\\xbf\\xbd',\n",
       " 253: b'\\xef\\xbf\\xbd',\n",
       " 254: b'\\xef\\xbf\\xbd',\n",
       " 255: b'\\xef\\xbf\\xbd',\n",
       " 256: b'\\xef\\xbf\\xbd',\n",
       " 257: b' t',\n",
       " 258: b' a',\n",
       " 259: b'he',\n",
       " 260: b'in',\n",
       " 261: b' the',\n",
       " 262: b're',\n",
       " 263: b' o',\n",
       " 264: b' ,',\n",
       " 265: b'er',\n",
       " 266: b' s',\n",
       " 267: b'at',\n",
       " 268: b' .',\n",
       " 269: b'nd',\n",
       " 270: b'is',\n",
       " 271: b'or',\n",
       " 272: b' w',\n",
       " 273: b' c',\n",
       " 274: b'on',\n",
       " 275: b' b',\n",
       " 276: b' f',\n",
       " 277: b'ou',\n",
       " 278: b'it',\n",
       " 279: b'en',\n",
       " 280: b'es',\n",
       " 281: b' of',\n",
       " 282: b' p',\n",
       " 283: b'ing',\n",
       " 284: b' in',\n",
       " 285: b'ed',\n",
       " 286: b'al',\n",
       " 287: b' m',\n",
       " 288: b' and',\n",
       " 289: b' d',\n",
       " 290: b'an',\n",
       " 291: b'ar',\n",
       " 292: b' to',\n",
       " 293: b'om',\n",
       " 294: b' th',\n",
       " 295: b'ic',\n",
       " 296: b'ion',\n",
       " 297: b' h',\n",
       " 298: b' l',\n",
       " 299: b' y',\n",
       " 300: b' e',\n",
       " 301: b'as',\n",
       " 302: b'ot',\n",
       " 303: b'il',\n",
       " 304: b' n',\n",
       " 305: b' u',\n",
       " 306: b'ent',\n",
       " 307: b' be',\n",
       " 308: b' &',\n",
       " 309: b' is',\n",
       " 310: b' you',\n",
       " 311: b'os',\n",
       " 312: b' re',\n",
       " 313: b'et',\n",
       " 314: b' for',\n",
       " 315: b'ut',\n",
       " 316: b'el',\n",
       " 317: b' g',\n",
       " 318: b'ay',\n",
       " 319: b'st',\n",
       " 320: b'ow',\n",
       " 321: b'le',\n",
       " 322: b'ce',\n",
       " 323: b'ad',\n",
       " 324: b' on',\n",
       " 325: b' I',\n",
       " 326: b'ver',\n",
       " 327: b've',\n",
       " 328: b' A',\n",
       " 329: b'ur',\n",
       " 330: b'ol',\n",
       " 331: b'ct',\n",
       " 332: b'qu',\n",
       " 333: b' that',\n",
       " 334: b'im',\n",
       " 335: b'all',\n",
       " 336: b'am',\n",
       " 337: b'ig',\n",
       " 338: b'ch',\n",
       " 339: b'ation',\n",
       " 340: b' P',\n",
       " 341: b'ith',\n",
       " 342: b'ir',\n",
       " 343: b' S',\n",
       " 344: b' it',\n",
       " 345: b' pr',\n",
       " 346: b'ap',\n",
       " 347: b' sh',\n",
       " 348: b' C',\n",
       " 349: b'th',\n",
       " 350: b' com',\n",
       " 351: b' @',\n",
       " 352: b' wh',\n",
       " 353: b'-@',\n",
       " 354: b' are',\n",
       " 355: b' @-@',\n",
       " 356: b'nt',\n",
       " 357: b'id',\n",
       " 358: b' with',\n",
       " 359: b' al',\n",
       " 360: b'op',\n",
       " 361: b' us',\n",
       " 362: b'ers',\n",
       " 363: b' as',\n",
       " 364: b'the',\n",
       " 365: b'and',\n",
       " 366: b'if',\n",
       " 367: b'ord',\n",
       " 368: b'od',\n",
       " 369: b' he',\n",
       " 370: b'ist',\n",
       " 371: b'quot',\n",
       " 372: b'ment',\n",
       " 373: b' M',\n",
       " 374: b' or',\n",
       " 375: b'ore',\n",
       " 376: b' G',\n",
       " 377: b' fr',\n",
       " 378: b'ill',\n",
       " 379: b'res',\n",
       " 380: b' st',\n",
       " 381: b'ess',\n",
       " 382: b'ld',\n",
       " 383: b' this',\n",
       " 384: b' 2',\n",
       " 385: b'art',\n",
       " 386: b' ;',\n",
       " 387: b' L',\n",
       " 388: b'ly',\n",
       " 389: b'ain',\n",
       " 390: b'ul',\n",
       " 391: b' de',\n",
       " 392: b' con',\n",
       " 393: b'est',\n",
       " 394: b'se',\n",
       " 395: b'apos',\n",
       " 396: b'ag',\n",
       " 397: b' from',\n",
       " 398: b' an',\n",
       " 399: b' we',\n",
       " 400: b' (',\n",
       " 401: b'00',\n",
       " 402: b'ter',\n",
       " 403: b' E',\n",
       " 404: b'em',\n",
       " 405: b'ave',\n",
       " 406: b' not',\n",
       " 407: b' )',\n",
       " 408: b' 1',\n",
       " 409: b' your',\n",
       " 410: b'oc',\n",
       " 411: b' can',\n",
       " 412: b' by',\n",
       " 413: b' D',\n",
       " 414: b' ne',\n",
       " 415: b' v',\n",
       " 416: b'igh',\n",
       " 417: b'ich',\n",
       " 418: b' all',\n",
       " 419: b'ri',\n",
       " 420: b' up',\n",
       " 421: b' r',\n",
       " 422: b' W',\n",
       " 423: b'ble',\n",
       " 424: b' they',\n",
       " 425: b' B',\n",
       " 426: b'un',\n",
       " 427: b' ye',\n",
       " 428: b' which',\n",
       " 429: b' O',\n",
       " 430: b'ke',\n",
       " 431: b' wor',\n",
       " 432: b' su',\n",
       " 433: b' F',\n",
       " 434: b' H',\n",
       " 435: b' have',\n",
       " 436: b'ate',\n",
       " 437: b' shall',\n",
       " 438: b' ch',\n",
       " 439: b'ect',\n",
       " 440: b'ity',\n",
       " 441: b' sp',\n",
       " 442: b'ress',\n",
       " 443: b'ight',\n",
       " 444: b' will',\n",
       " 445: b' comp',\n",
       " 446: b'ort',\n",
       " 447: b'ant',\n",
       " 448: b' &#',\n",
       " 449: b'ive',\n",
       " 450: b'are',\n",
       " 451: b'..',\n",
       " 452: b' ex',\n",
       " 453: b' And',\n",
       " 454: b' ...',\n",
       " 455: b'ast',\n",
       " 456: b'24',\n",
       " 457: b' T',\n",
       " 458: b'ould',\n",
       " 459: b'ven',\n",
       " 460: b' tr',\n",
       " 461: b'ust',\n",
       " 462: b'um',\n",
       " 463: b'out',\n",
       " 464: b'com',\n",
       " 465: b' unt',\n",
       " 466: b' se',\n",
       " 467: b'ft',\n",
       " 468: b'ree',\n",
       " 469: b'ost',\n",
       " 470: b'og',\n",
       " 471: b'ish',\n",
       " 472: b'ions',\n",
       " 473: b'iz',\n",
       " 474: b'124',\n",
       " 475: b' unto',\n",
       " 476: b'mer',\n",
       " 477: b'ings',\n",
       " 478: b' ac',\n",
       " 479: b'this',\n",
       " 480: b'ated',\n",
       " 481: b'ac',\n",
       " 482: b'lu',\n",
       " 483: b'ere',\n",
       " 484: b' man',\n",
       " 485: b'for',\n",
       " 486: b' my',\n",
       " 487: b' at',\n",
       " 488: b'ies',\n",
       " 489: b'age',\n",
       " 490: b'rou',\n",
       " 491: b'lo',\n",
       " 492: b'ans',\n",
       " 493: b'pp',\n",
       " 494: b'ind',\n",
       " 495: b' work',\n",
       " 496: b'here',\n",
       " 497: b'fore',\n",
       " 498: b' sit',\n",
       " 499: b' ver'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f240948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d1b5e517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8670d659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the bananas are green'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\" the bananas are green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "539b0ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['owt_train.txt',\n",
       " 'TinyStoriesV2-GPT4-train.txt',\n",
       " 'TinyStoriesV2-GPT4-valid.txt']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7705b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = \"<|endoftext|>\"\n",
    "with open(\"../data/TinyStoriesV2-GPT4-valid.txt\", \"rb\") as f:\n",
    "    doc = f.read().split(special_token.encode())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "56d7f554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jkj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8473c7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '\\x00'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(json.dumps({1:vocab[1].decode()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822f082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381d2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd3ef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "# for (p0, p1), idx in merges.items():\n",
    "#     vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b832aa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(bytes([28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7620aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'D'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes([68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b25171b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5aaba69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(b'&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e02c5887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324b396",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "- `train_bpe_tinystoires`\n",
    "    - Current memory usage: 5.24 MB\n",
    "    - Peak memory usage: 116.74 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18274d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'a' in (b'a' + b'\\x80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ea20e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'ab'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'abc'[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf464893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98, 99]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(b'bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f74380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'eb'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'ab'.replace(b'a', b'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74e2583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "def _update_byte_tuple(byte_tuple: Iterable[bytes], merge_loc: int):\n",
    "    \"\"\"\n",
    "    Merge the byte tuple at the merge location.\n",
    "    \"\"\"\n",
    "    assert len(byte_tuple) > 1, \"Cannot merge a byte tuple with length less than 2.\"\n",
    "    prefix = byte_tuple[:merge_loc]\n",
    "    tomerge = byte_tuple[merge_loc:merge_loc+2]\n",
    "    suffix = byte_tuple[merge_loc+2:]\n",
    "    new_byte_tuple = prefix + (b\"\".join(tomerge),) + suffix\n",
    "    return new_byte_tuple, prefix, suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85ca5bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((b'x', b'yz'), (b'x',), ())"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_tuple = tuple(bytes([c]) for c in 'xyz'.encode())\n",
    "# tuple(bytes([b]) for b in pretoken)\n",
    "_update_byte_tuple(byte_tuple, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "579ce616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'x', b'y', b'z')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "839df03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 121, 122)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(b'xyz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
